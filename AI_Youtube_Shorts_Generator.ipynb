{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y imagemagick\n",
    "!sed -i 's/<policy domain=\"path\" rights=\"none\" pattern=\"@\\*\" \\/>/<!-- <policy domain=\"path\" rights=\"none\" pattern=\"@\\*\" \\/> -->/g' /etc/ImageMagick-6/policy.xml\n",
    "!echo 'faster_whisper==1.0.1' > requirements.txt\n",
    "!echo 'ffmpeg==1.4' >> requirements.txt\n",
    "!echo 'ffmpeg_python==0.2.0' >> requirements.txt\n",
    "!echo 'moviepy==1.0.3' >> requirements.txt\n",
    "!echo 'numpy<2.0' >> requirements.txt\n",
    "!echo 'opencv_python==4.7.0.72' >> requirements.txt\n",
    "!echo 'opencv_python_headless==4.9.0.80' >> requirements.txt\n",
    "!echo 'python-dotenv==1.0.1' >> requirements.txt\n",
    "!echo 'pytubefix' >> requirements.txt\n",
    "!echo 'torch' >> requirements.txt\n",
    "!echo 'webrtcvad-wheels' >> requirements.txt\n",
    "!echo 'pydub' >> requirements.txt\n",
    "!echo 'openai==0.28.1' >> requirements.txt\n",
    "!echo 'google-generativeai' >> requirements.txt\n",
    "!echo 'pysrt' >> requirements.txt\n",
    "!echo '--extra-index-url https://download.pytorch.org/whl/cu121' >> requirements.txt\n",
    "!pip install gradio\n",
    "!pip install -r requirements.txt\n",
    "!mkdir -p videos\n",
    "!mkdir -p models\n",
    "!wget -O models/deploy.prototxt https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/models/deploy.prototxt\n",
    "!wget -O models/res10_300x300_ssd_iter_140000_fp16.caffemodel https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/models/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "!wget -O haarcascade_frontalface_default.xml https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import wave\n",
    "import contextlib\n",
    "from pydub import AudioSegment\n",
    "from pytubefix import YouTube\n",
    "import ffmpeg\n",
    "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, ImageClip\n",
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "from faster_whisper import WhisperModel\n",
    "import torch\n",
    "import openai\n",
    "import json\n",
    "import zipfile\n",
    "import gradio as gr\n",
    "import shutil\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def parse_srt(srt_file_path):\n",
    "    with open(srt_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    dialogue_lines = [line.strip() for line in lines if not re.match(r'\\d+', line.strip()) and '-->' not in line and line.strip() != '']\n",
    "    return ' '.join(dialogue_lines)\n",
    "\n",
    "# --- YouTubeDownloader --- \n",
    "def download_youtube_video(url, logger, progress):\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        logger(f'Downloading video: {yt.title}')\n",
    "        progress(0.1, desc=f'Downloading video: {yt.title}')\n",
    "        video_stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "        if not video_stream:\n",
    "            video_stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()\n",
    "\n",
    "        if not os.path.exists('videos'):\n",
    "            os.makedirs('videos')\n",
    "\n",
    "        video_file = video_stream.download(output_path='videos')\n",
    "        logger(f'Downloaded: {yt.title} to videos folder')\n",
    "        return video_file\n",
    "    except Exception as e:\n",
    "        logger(f'An error occurred during download: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# --- Edit ---\n",
    "def extractAudio(video_path, logger, progress):\n",
    "    try:\n",
    "        logger('Extracting audio...')\n",
    "        progress(0.2, desc='Extracting audio...')\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        audio_path = 'audio.wav'\n",
    "        video_clip.audio.write_audiofile(audio_path)\n",
    "        video_clip.close()\n",
    "        logger(f'Extracted audio to: {audio_path}')\n",
    "        return audio_path\n",
    "    except Exception as e:\n",
    "        logger(f'An error occurred while extracting audio: {e}')\n",
    "        return None\n",
    "\n",
    "def crop_video(input_file, output_file, start_time, end_time, logger):\n",
    "    logger(f'Cropping video from {start_time} to {end_time}')\n",
    "    with VideoFileClip(input_file) as video:\n",
    "        cropped_video = video.subclip(start_time, end_time)\n",
    "        cropped_video.write_videofile(output_file, codec='libx264')\n",
    "    logger(f'Video {output_file} cropped successfully.')\n",
    "\n",
    "# --- Transcription ---\n",
    "def transcribeAudio(audio_path, logger, progress):\n",
    "    try:\n",
    "        logger('Transcribing audio...')\n",
    "        progress(0.3, desc='Transcribing audio...')\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger(f'Using device: {device}')\n",
    "        model = WhisperModel('base.en', device=device, local_files_only=False)\n",
    "        logger('Whisper model loaded.')\n",
    "        segments, info = model.transcribe(audio=audio_path, beam_size=5, language='en')\n",
    "        segments = list(segments)\n",
    "        logger('Transcription complete.')\n",
    "        progress(0.4, desc='Transcription complete.')\n",
    "        trans_text = ''\n",
    "        for seg in segments:\n",
    "            trans_text += f'{seg.start} - {seg.end}: {seg.text.strip()}\\n'\n",
    "        return trans_text\n",
    "    except Exception as e:\n",
    "        logger(f'Transcription Error: {e}')\n",
    "        return None\n",
    "\n",
    "# --- LanguageTasks ---\n",
    "def get_highlights(transcription, api_key, logger, progress, provider, max_clips, min_length, max_length):\n",
    "    logger(f'Getting up to {max_clips} highlights from transcription using {provider}...')\n",
    "    progress(0.5, desc=f'Getting highlights from transcription using {provider}...')\n",
    "    \n",
    "    system_prompt = f'''\n",
    "    You are an expert viral video clip finder. Based on the provided transcription, identify up to {max_clips} of the most interesting, engaging, or viral-worthy segments. \n",
    "    Each segment must be between {min_length} and {max_length} seconds long.\n",
    "    Return the results as a valid JSON array of objects. Each object must have 'start' and 'end' keys with the timestamps in seconds, and a 'content' key with the summary of the highlight.\n",
    "    \n",
    "    Example format:\n",
    "    [\n",
    "      {{\n",
    "        \"start\": 42.5,\n",
    "        \"end\": 88.0,\n",
    "        \"content\": \"A summary of the first highlight.\"\n",
    "      }},\n",
    "      {{\n",
    "        \"start\": 123.1,\n",
    "        \"end\": 160.7,\n",
    "        \"content\": \"A summary of the second highlight.\"\n",
    "      }}\n",
    "    ]\n",
    "    \n",
    "    Do not say anything else, just return the proper JSON. If you cannot find any highlights that fit the criteria, return an empty array [].\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        if provider == 'OpenAI':\n",
    "            openai.api_key = api_key\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model='gpt-4o-2024-05-13',\n",
    "                temperature=0.7,\n",
    "                messages=[\n",
    "                    {'role': 'system', 'content': system_prompt},\n",
    "                    {'role': 'user', 'content': transcription},\n",
    "                ],\n",
    "            )\n",
    "            json_string = response.choices[0].message.content\n",
    "        elif provider == 'Gemini':\n",
    "            genai.configure(api_key=api_key)\n",
    "            model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "            response = model.generate_content(system_prompt + '\\n\\n' + transcription)\n",
    "            json_string = response.text\n",
    "        else:\n",
    "            raise ValueError('Invalid AI provider selected')\n",
    "            \n",
    "        json_string = json_string.strip().replace('json', '').replace('```', '')\n",
    "        highlights = json.loads(json_string)\n",
    "        logger(f'Found {len(highlights)} highlights.')\n",
    "        return highlights\n",
    "    except Exception as e:\n",
    "        logger(f'Error in GetHighlight: {e}')\n",
    "        return []\n",
    "\n",
    "# --- Finalization ---\n",
    "def burn_srt_to_video(video_path, srt_path, output_path, logger):\n",
    "    logger(f'Burning SRT to video: {output_path}')\n",
    "    video = VideoFileClip(video_path)\n",
    "    # It's better to create a custom subtitle clip for this\n",
    "    # For now, we'll assume the srt_file provided is for the whole video\n",
    "    # A more advanced version would create a new SRT for the clip's duration\n",
    "    generator = lambda txt: TextClip(txt, font='Arial', fontsize=24, color='white', bg_color='black')\n",
    "    subtitles = SubtitlesClip(srt_path, generator).set_pos(('center','bottom'))\n",
    "    result = CompositeVideoClip([video, subtitles])\n",
    "    result.write_videofile(output_path, codec='libx264')\n",
    "    logger('SRT burned to video.')\n",
    "\n",
    "def add_watermark(video_path, watermark_path, output_path, logger):\n",
    "    logger(f'Adding watermark to: {output_path}')\n",
    "    video = VideoFileClip(video_path)\n",
    "    watermark = (ImageClip(watermark_path)\n",
    "                 .set_duration(video.duration)\n",
    "                 .resize(height=50) # Tweak size as needed\n",
    "                 .margin(right=8, top=8, opacity=0) # Tweak position as needed\n",
    "                 .set_pos(('right','top')))\n",
    "    result = CompositeVideoClip([video, watermark])\n",
    "    result.write_videofile(output_path, codec='libx264')\n",
    "    logger('Watermark added.')\n",
    "\n",
    "# --- Main Processing Function ---\n",
    "def generate_shorts(video_file, srt_file, provider, openai_api_key, gemini_api_key, min_length, max_length, max_clips, aspect_ratio, burn_srt, export_srt, output_name, watermark_file, progress=gr.Progress()):\n",
    "    logs = []\n",
    "    def logger(message):\n",
    "        logs.append(message)\n",
    "        print(message)\n",
    "\n",
    "    logger('Starting short generation process...')\n",
    "    progress(0, desc='Starting...')\n",
    "\n",
    "    video_path = video_file.name\n",
    "    if not video_path:\n",
    "        logger('Video upload failed.')\n",
    "        return None, '\\n'.join(logs)\n",
    "\n",
    "    trans_text = None\n",
    "    if srt_file is not None:\n",
    "        logger('SRT file provided. Skipping transcription.')\n",
    "        progress(0.4, desc='Parsing SRT file...')\n",
    "        trans_text = parse_srt(srt_file.name)\n",
    "    else:\n",
    "        logger('No SRT file provided. Starting automatic transcription.')\n",
    "        audio_path = extractAudio(video_path, logger, progress)\n",
    "        if not audio_path:\n",
    "            logger('Audio extraction failed.')\n",
    "            return None, '\\n'.join(logs)\n",
    "        trans_text = transcribeAudio(audio_path, logger, progress)\n",
    "\n",
    "    if not trans_text:\n",
    "        logger('Failed to get transcription from either SRT or Whisper.')\n",
    "        return None, '\\n'.join(logs)\n",
    "\n",
    "    api_key = openai_api_key if provider == 'OpenAI' else gemini_api_key\n",
    "    highlights = get_highlights(trans_text, api_key, logger, progress, provider, max_clips, min_length, max_length)\n",
    "\n",
    "    if not highlights:\n",
    "        logger('Failed to get any highlights from the AI. Try adjusting the length parameters or using a different video.')\n",
    "        return None, '\\n'.join(logs)\n",
    "\n",
    "    output_filename_prefix = output_name if output_name else 'short'\n",
    "    final_files = []\n",
    "\n",
    "    for i, highlight in enumerate(highlights):\n",
    "        start_time = highlight['start']\n",
    "        end_time = highlight['end']\n",
    "        logger(f'--- Processing clip {i+1}/{len(highlights)}: {start_time}s - {end_time}s ---')\n",
    "        progress((i+1)/len(highlights), desc=f'Processing clip {i+1}/{len(highlights)}')\n",
    "\n",
    "        clip_path = f'{output_filename_prefix}_{i+1}.mp4'\n",
    "        crop_video(video_path, clip_path, start_time, end_time, logger)\n",
    "\n",
    "        if burn_srt and srt_file:\n",
    "            subtitled_path = f'{output_filename_prefix}_{i+1}_subtitled.mp4'\n",
    "            burn_srt_to_video(clip_path, srt_file.name, subtitled_path, logger)\n",
    "            clip_path = subtitled_path\n",
    "\n",
    "        if watermark_file:\n",
    "            watermarked_path = f'{output_filename_prefix}_{i+1}_watermarked.mp4'\n",
    "            add_watermark(clip_path, watermark_file.name, watermarked_path, logger)\n",
    "            clip_path = watermarked_path\n",
    "        \n",
    "        final_files.append(clip_path)\n",
    "    \n",
    "    logger(f'Generated {len(final_files)} clips.')\n",
    "    zip_path = f'{output_filename_prefix}_results.zip'\n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for file in final_files:\n",
    "            zipf.write(file)\n",
    "        if export_srt and srt_file:\n",
    "            zipf.write(srt_file.name)\n",
    "        with open('transcription.txt', 'w') as f:\n",
    "            f.write(trans_text)\n",
    "        zipf.write('transcription.txt')\n",
    "        \n",
    "    logger('Process finished.')\n",
    "    progress(1, desc='Finished!')\n",
    "    \n",
    "    return zip_path, '\\n'.join(logs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('# AI Youtube Shorts Generator')\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            video_file = gr.File(label='Upload Long Video')\n",
    "            srt_file = gr.File(label='Upload SRT File (Optional)')\n",
    "            provider = gr.Dropdown(['OpenAI', 'Gemini'], label='AI Provider', value='OpenAI')\n",
    "            openai_api_key = gr.Textbox(label='OpenAI API Key', type='password', visible=True)\n",
    "            gemini_api_key = gr.Textbox(label='Gemini API Key', type='password', visible=False)\n",
    "            with gr.Row():\n",
    "                min_length = gr.Slider(10, 120, value=30, label='Minimum length (s)')\n",
    "                max_length = gr.Slider(15, 180, value=60, label='Maximum length (s)')\n",
    "            max_clips = gr.Slider(1, 10, value=3, step=1, label='Max Number of Shorts')\n",
    "            aspect_ratio = gr.Dropdown(['9:16', '16:9', '1:1'], label='Aspect Ratio', value='9:16')\n",
    "            with gr.Row():\n",
    "                burn_srt = gr.Checkbox(label='Burn SRT into Video')\n",
    "                export_srt = gr.Checkbox(label='Export SRT Separately')\n",
    "            output_name = gr.Textbox(label='Output File Name Prefix', placeholder='e.g., my_awesome_short')\n",
    "            watermark_file = gr.File(label='Upload Watermark (Optional)')\n",
    "            generate_button = gr.Button('Generate Shorts')\n",
    "        with gr.Column():\n",
    "            logs = gr.Textbox(label='Logs', lines=15, interactive=False)\n",
    "            output_zip = gr.File(label='Download Results ZIP')\n",
    "\n",
    "    def toggle_api_key(provider_choice):\n",
    "        if provider_choice == 'OpenAI':\n",
    "            return gr.update(visible=True), gr.update(visible=False)\n",
    "        else:\n",
    "            return gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "    provider.change(toggle_api_key, provider, [openai_api_key, gemini_api_key])\n",
    "\n",
    "    generate_button.click(\n",
    "        fn=generate_shorts,\n",
    "        inputs=[\n",
    "            video_file,\n",
    "            srt_file,\n",
    "            provider,\n",
    "            openai_api_key,\n",
    "            gemini_api_key,\n",
    "            min_length,\n",
    "            max_length,\n",
    "            max_clips,\n",
    "            aspect_ratio,\n",
    "            burn_srt,\n",
    "            export_srt,\n",
    "            output_name,\n",
    "            watermark_file,\n",
    "        ],\n",
    "        outputs=[output_zip, logs]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
