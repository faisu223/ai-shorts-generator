{
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":["# AI YouTube Shorts Generator (Colab-Ready)\n","Run all cells top-to-bottom. In Colab, set Runtime -> GPU for best speed.\n"]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# Setup: system packages + Python deps\n",
      "!apt-get update -qq\n",
      "!apt-get install -y -qq imagemagick ffmpeg fonts-freefont-ttf > /dev/null\n",
      "!sed -i 's/<policy domain=\"path\" rights=\"none\" pattern=\"@\\*\" \/>/<!-- <policy domain=\"path\" rights=\"none\" pattern=\"@\\*\" \/> -->/g' /etc/ImageMagick-6/policy.xml || true\n",
      "%pip -q install --upgrade pip wheel setuptools\n",
      "%pip -q install gradio==4.* moviepy==1.0.3 imageio-ffmpeg\n",
      "%pip -q install numpy<2.0 opencv-python-headless 'pytubefix' pydub pysrt faster-whisper webrtcvad-wheels google-generativeai\n",
      "%pip -q install 'openai>=1.35.0'\n",
      "import torch, os\n",
      "if torch.cuda.is_available():\n",
      "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
      "    %pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
      "else:\n",
      "    %pip -q install torch torchvision torchaudio\n",
      "!mkdir -p videos models outputs\n",
      "!wget -q -O models/deploy.prototxt https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/models/deploy.prototxt\n",
      "!wget -q -O models/res10_300x300_ssd_iter_140000_fp16.caffemodel https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/models/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
      "!wget -q -O models/haarcascade_frontalface_default.xml https://raw.githubusercontent.com/faisu223/Faisal/feature/gradio-interface/haarcascade_frontalface_default.xml\n",
      "print('Setup complete. CUDA:', torch.cuda.is_available())\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import os, re, json, zipfile, math\n",
      "import cv2, numpy as np\n",
      "from typing import List, Dict, Tuple, Optional\n",
      "import gradio as gr\n",
      "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, ImageClip\n",
      "from moviepy.video.fx.all import crop as mp_crop\n",
      "from moviepy.video.tools.subtitles import SubtitlesClip\n",
      "from faster_whisper import WhisperModel\n",
      "from pytubefix import YouTube\n",
      "import google.generativeai as genai\n",
      "try:\n",
      "    from openai import OpenAI as _OpenAI\n",
      "    _OPENAI_NEW=True\n",
      "except Exception:\n",
      "    import openai as _openai_legacy\n",
      "    _OPENAI_NEW=False\n",
      "\n",
      "def parse_srt_segments(path:str)->List[Dict]:\n",
      "    content=open(path,'r',encoding='utf-8',errors='ignore').read()\n",
      "    blocks=re.split(r'\n\s*\n',content.strip())\n",
      "    segs=[]\n",
      "    for b in blocks:\n",
      "        lines=[l.strip('\ufeff ') for l in b.splitlines() if l.strip()]\n",
      "        if not lines: continue\n",
      "        tl=None\n",
      "        for l in lines:\n",
      "            if '-->' in l: tl=l; break\n",
      "        if not tl: continue\n",
      "        try:\n",
      "            t0,t1=[x.strip() for x in tl.split('-->')]\n",
      "            def to_s(ts):\n",
      "                h,m,rest=ts.split(':'); s,ms=(rest+',0').split(',')[:2]; return int(h)*3600+int(m)*60+int(s)+int(ms)/1000\n",
      "            st,et=to_s(t0),to_s(t1)\n",
      "            if et>st:\n",
      "                text=' '.join([l for l in lines if l!=tl and not l.isdigit()])\n",
      "                segs.append({'start':st,'end':et,'text':text})\n",
      "        except: pass\n",
      "    return segs\n",
      "\n",
      "def segs_to_text(segs:List[Dict])->str:\n",
      "    return ' '.join(s.get('text','').strip() for s in segs)\n",
      "\n",
      "def srt_time(t:float)->str:\n",
      "    t=max(0,float(t)); h=int(t//3600); m=int((t%3600)//60); s=int(t%60); ms=int((t-int(t))*1000); return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"\n",
      "\n",
      "def write_srt_for_clip(segs:List[Dict], path:str, t0:float, t1:float):\n",
      "    idx=1\n",
      "    with open(path,'w',encoding='utf-8') as f:\n",
      "        for s in segs:\n",
      "            if s['end']<t0 or s['start']>t1: continue\n",
      "            a=max(t0,s['start'])-t0; b=min(t1,s['end'])-t0\n",
      "            if b<=a: continue\n",
      "            txt=s['text'].replace('\n',' ').strip()\n",
      "            f.write(f"{idx}\n{srt_time(a)} --> {srt_time(b)}\n{txt}\n\n")\n",
      "            idx+=1\n",
      "\n",
      "def download_youtube(url:str, logger, progress)->Optional[str]:\n",
      "    try:\n",
      "        yt=YouTube(url); logger(f'Downloading: {yt.title}'); progress(0.08,desc='Downloading YouTube...')\n",
      "        st=(yt.streams.filter(progressive=True,file_extension='mp4').order_by('resolution').desc().first()\n",
      "            or yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first())\n",
      "        os.makedirs('videos',exist_ok=True); return st.download(output_path='videos')\n",
      "    except Exception as e:\n",
      "        logger(f'YouTube error: {e}'); return None\n",
      "\n",
      "def extract_audio(video_path:str, logger, progress)->Optional[str]:\n",
      "    try:\n",
      "        progress(0.18,desc='Extracting audio...')\n",
      "        with VideoFileClip(video_path) as v: ap=v.audio; ap.write_audiofile('audio.wav'); ap.close()\n",
      "        return 'audio.wav'\n",
      "    except Exception as e:\n",
      "        logger(f'Audio error: {e}'); return None\n",
      "\n",
      "def transcribe(video_path:str, logger, progress)->Tuple[List[Dict],str]:\n",
      "    try:\n",
      "        device='cuda' if (hasattr(__import__('torch'),'cuda') and __import__('torch').cuda.is_available()) else 'cpu'\n",
      "        model=WhisperModel('base.en', device=device, compute_type='float16' if device=='cuda' else 'int8')\n",
      "        progress(0.3,desc='Transcribing...')\n",
      "        seg_iter,_=model.transcribe(video_path, beam_size=5, language='en')\n",
      "        segs=[{'start':float(s.start),'end':float(s.end),'text':s.text.strip()} for s in seg_iter]\n",
      "        return segs, segs_to_text(segs)\n",
      "    except Exception as e:\n",
      "        logger(f'Transcription error: {e}'); return [], ''\n",
      "\n",
      "def aspect_tuple(s:str)->Tuple[int,int]:\n",
      "    a,b=s.split(':'); return int(a),int(b)\n",
      "\n",
      "def crop_center(v:VideoFileClip, ratio:str)->VideoFileClip:\n",
      "    aw,ah=aspect_tuple(ratio); tr=aw/ah; w,h=v.w,v.h; sr=w/h\n",
      "    if sr>tr: cw=int(h*tr); ch=h; x=(w-cw)//2; y=0\n",
      "    else: cw=w; ch=int(w/tr); x=0; y=(h-ch)//2\n",
      "    return mp_crop(v, x1=x, y1=y, width=cw, height=ch).resize((cw, ch))

def load_face_detectors():
    dnn_proto = 'models/deploy.prototxt'
    dnn_model = 'models/res10_300x300_ssd_iter_140000_fp16.caffemodel'
    dnn = None
    if os.path.exists(dnn_proto) and os.path.exists(dnn_model):
        try:
            dnn = cv2.dnn.readNetFromCaffe(dnn_proto, dnn_model)
        except Exception:
            dnn = None
    haar = None
    haar_path = 'models/haarcascade_frontalface_default.xml'
    if os.path.exists(haar_path):
        haar = cv2.CascadeClassifier(haar_path)
    return dnn, haar

DNN_FACE, HAAR_FACE = load_face_detectors()

def detect_faces_frame(frame):
    h, w = frame.shape[:2]
    faces = []
    if DNN_FACE is not None:
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300,300)), 1.0, (300,300), (104.0,177.0,123.0))
        DNN_FACE.setInput(blob)
        detections = DNN_FACE.forward()
        for i in range(detections.shape[2]):
            conf = detections[0,0,i,2]
            if conf > 0.6:
                box = detections[0,0,i,3:7] * np.array([w,h,w,h])
                x1, y1, x2, y2 = box.astype(int)
                faces.append((max(0,x1), max(0,y1), min(w,x2)-max(0,x1), min(h,y2)-max(0,y1)))
    if not faces and HAAR_FACE is not None:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        dets = HAAR_FACE.detectMultiScale(gray, 1.2, 3)
        for (x,y,fw,fh) in dets:
            faces.append((x,y,fw,fh))
    return faces

def compute_crop_for_aspect(w:int, h:int, ratio:str, bbox=None):
    aw, ah = aspect_tuple(ratio)
    tr = aw/ah
    sr = w/h
    if sr > tr:
        ch = h
        cw = int(h*tr)
    else:
        cw = w
        ch = int(w/tr)
    cx, cy = w//2, h//2
    if bbox is not None:
        x, y, bw, bh = bbox
        cx = x + bw//2
        cy = y + bh//2
    x1 = max(0, min(w - cw, cx - cw//2))
    y1 = max(0, min(h - ch, cy - ch//2))
    return x1, y1, cw, ch

def crop_face_aware(v:VideoFileClip, ratio:str)->VideoFileClip:
    try:
        t = max(0.0, min(v.duration, v.duration/2.0))
        frame = v.get_frame(t)
        faces = detect_faces_frame(frame)
        if faces:
            faces.sort(key=lambda b: b[2]*b[3], reverse=True)
            x, y, w, h = faces[0]
            x1, y1, cw, ch = compute_crop_for_aspect(v.w, v.h, ratio, (x,y,w,h))
            return mp_crop(v, x1=x1, y1=y1, width=cw, height=ch).resize((cw, ch))
    except Exception:
        pass
    return crop_center(v, ratio)\n",
      "\n",
      "def burn_subs(video_path:str, srt_path:str, out_path:str, logger):\n",
      "    gen=lambda txt: TextClip(txt, font='FreeMono', fontsize=40, color='white', stroke_color='black', stroke_width=2)\n",
      "    v=VideoFileClip(video_path); subs=SubtitlesClip(srt_path, gen).set_pos(('center','bottom'))\n",
      "    CompositeVideoClip([v,subs]).write_videofile(out_path, codec='libx264', audio_codec='aac'); v.close()\n",
      "\n",
      "def add_watermark(video_path:str, wm_path:str, out_path:str):\n",
      "    v=VideoFileClip(video_path); wm=(ImageClip(wm_path).set_duration(v.duration).resize(height=64).set_pos(('right','top')))\n",
      "    CompositeVideoClip([v,wm]).write_videofile(out_path, codec='libx264', audio_codec='aac'); v.close()\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def pick_highlights(transcription:str, provider:str, api_key:str, logger, progress, max_clips:int, min_len:int, max_len:int)->List[Dict]:\n",
      "    sys=f'You are an expert at finding viral video moments. Return up to {max_clips} segments between {min_len} and {max_len} seconds as JSON array with keys start,end,content. Only return JSON. If none, return [].'\n",
      "    try:\n",
      "        progress(0.5,desc=f'AI selecting highlights ({provider})...')\n",
      "        if provider=='OpenAI':\n",
      "            if _OPENAI_NEW:\n",
      "                client=_OpenAI(api_key=api_key)\n",
      "                r=client.chat.completions.create(model='gpt-4o-mini',temperature=0.5,messages=[{'role':'system','content':sys},{'role':'user','content':transcription}])\n",
      "                txt=r.choices[0].message.content\n",
      "            else:\n",
      "                _openai_legacy.api_key=api_key\n",
      "                r=_openai_legacy.ChatCompletion.create(model='gpt-4o-2024-05-13',temperature=0.5,messages=[{'role':'system','content':sys},{'role':'user','content':transcription}])\n",
      "                txt=r.choices[0].message.content\n",
      "        else:\n",
      "            genai.configure(api_key=api_key); m=genai.GenerativeModel('gemini-1.5-flash'); txt=m.generate_content(sys+'\n\n'+transcription).text\n",
      "        txt=(txt or '').strip().replace('```','').replace('json','').strip()\n",
      "        arr=json.loads(txt) if txt else []\n",
      "        out=[]\n",
      "        for h in arr:\n",
      "            try:\n",
      "                s=float(h.get('start',0)); e=float(h.get('end',0));\n",
      "                if e>s and min_len<=e-s<=max_len: out.append({'start':s,'end':e,'content':h.get('content','')})\n",
      "            except: pass\n",
      "        logger(f'Highlights: {len(out)}'); return out\n",
      "    except Exception as e:\n",
      "        logger(f'Highlight error: {e}'); return []\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def generate_shorts(youtube_url, video_file, srt_file, provider, openai_key, gemini_key, min_len, max_len, max_clips, aspect, crop_mode, burn_srt, export_srt, out_prefix, watermark_file, progress=gr.Progress()):\n",
      "    logs=[]\n",
      "    def log(m): logs.append(m); print(m)\n",
      "    progress(0, desc='Starting...')\n",
      "    path=None\n",
      "    if youtube_url: path=download_youtube(youtube_url, log, progress)\n",
      "    if not path and video_file is not None: path=video_file.name\n",
      "    if not path: log('No video provided.'); return None, '\n'.join(logs)\n",
      "    segs=[]; text=''\n",
      "    if srt_file is not None:\n",
      "        log('Parsing provided SRT...'); segs=parse_srt_segments(srt_file.name); text=segs_to_text(segs)\n",
      "    else:\n",
      "        log('Transcribing...'); segs, text = transcribe(path, log, progress)\n",
      "    if not text: log('No transcription text.'); return None, '\n'.join(logs)\n",
      "    api_key = openai_key if provider=='OpenAI' else gemini_key\n",
      "    highs = pick_highlights(text, provider, api_key, log, progress, int(max_clips), int(min_len), int(max_len))\n",
      "    if not highs: log('No highlights found.'); return None, '\n'.join(logs)\n",
      "    out_pref = out_prefix or 'short'\n",
      "    outputs=[]\n",
      "    for i,h in enumerate(highs, start=1):\n",
      "        s,e=float(h['start']), float(h['end'])\n",
      "        clip_path=f"{out_pref}_{i}.mp4"\n",
      "        with VideoFileClip(path) as v:\n",
      "            sub=v.subclip(s,e)\n",
      "            sub = crop_face_aware(sub, aspect) if (crop_mode == 'Face-aware') else crop_center(sub, aspect)\n",
      "            sub.write_videofile(clip_path, codec='libx264', audio_codec='aac')\n",
      "        cur=clip_path\n",
      "        if burn_srt and segs:\n",
      "            tmp_srt=f"{out_pref}_{i}.srt"\n",
      "            write_srt_for_clip(segs, tmp_srt, s, e)\n",
      "            out_srted=f"{out_pref}_{i}_sub.mp4"\n",
      "            try: burn_subs(cur, tmp_srt, out_srted, log); cur=out_srted\n",
      "            except Exception as ex: log(f'Subtitle burn failed: {ex}')\n",
      "        if watermark_file is not None:\n",
      "            wm_out=f"{out_pref}_{i}_wm.mp4"\n",
      "            try: add_watermark(cur, watermark_file.name, wm_out); cur=wm_out\n",
      "            except Exception as ex: log(f'Watermark failed: {ex}')\n",
      "        outputs.append(cur)\n",
      "    zip_path=f"{out_pref}_results.zip"\n",
      "    with zipfile.ZipFile(zip_path,'w') as z:\n",
      "        for f in outputs: z.write(f)\n",
      "        if export_srt and segs:\n",
      "            full_srt=f"{out_pref}_full.srt"; write_srt_for_clip(segs, full_srt, 0, 10**9); z.write(full_srt)\n",
      "        with open('transcription.txt','w',encoding='utf-8') as f: f.write(text)\n",
      "        z.write('transcription.txt')\n",
      "    progress(1, desc='Done')\n",
      "    return zip_path, '\n'.join(logs)\n"
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "with gr.Blocks() as demo:\n",
      "    gr.Markdown('# AI YouTube Shorts Generator')\n",
      "    with gr.Row():\n",
      "        with gr.Column(scale=1):\n",
      "            youtube_url = gr.Textbox(label='YouTube URL (optional)')\n",
      "            video_file = gr.File(label='Or upload a video')\n",
      "            srt_file = gr.File(label='Upload SRT (optional)')\n",
      "            provider = gr.Dropdown(['OpenAI','Gemini'], label='AI Provider', value='OpenAI')\n",
      "            openai_key = gr.Textbox(label='OpenAI API Key', type='password', visible=True)\n",
      "            gemini_key = gr.Textbox(label='Gemini API Key', type='password', visible=False)\n",
      "            with gr.Row():\n",
      "                min_len = gr.Slider(5,120,value=25,step=1,label='Min length (s)')\n",
      "                max_len = gr.Slider(10,180,value=60,step=1,label='Max length (s)')\n",
      "            max_clips = gr.Slider(1,10,value=3,step=1,label='Max clips')\n",
      "            aspect = gr.Dropdown(['9:16','16:9','1:1'], value='9:16', label='Aspect ratio')
            crop_mode = gr.Dropdown(['Center','Face-aware'], value='Center', label='Crop mode')\n",
      "            with gr.Row():\n",
      "                burn_srt = gr.Checkbox(label='Burn subtitles')\n",
      "                export_srt = gr.Checkbox(label='Export full SRT in ZIP')\n",
      "            out_prefix = gr.Textbox(label='Output name prefix', placeholder='short')\n",
      "            watermark_file = gr.File(label='Watermark image (optional)')\n",
      "            go = gr.Button('Generate Shorts')\n",
      "        with gr.Column(scale=1):\n",
      "            logs = gr.Textbox(label='Logs', lines=18)\n",
      "            out_zip = gr.File(label='Results ZIP')\n",
      "    def _toggle(p):\n",
      "        return (gr.update(visible=p=='OpenAI'), gr.update(visible=p=='Gemini'))\n",
      "    provider.change(_toggle, inputs=provider, outputs=[openai_key, gemini_key])\n",
      "    go.click(generate_shorts, [youtube_url, video_file, srt_file, provider, openai_key, gemini_key, min_len, max_len, max_clips, aspect, crop_mode, burn_srt, export_srt, out_prefix, watermark_file], [out_zip, logs])\n",
      "demo.launch(debug=True, share=True)\n"
    ]}
  ],
  "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
  "nbformat": 4,
  "nbformat_minor": 5
}